[
  {
    "provider": "Mistral",
    "model": "Mistral Nemo",
    "version": "NeMo 12B",
    "description": "NeMo conversational model, optimized for chat usage.",
    "model_size": 12,
    "input_cost": 0.3,
    "output_cost": 0.3,
    "cache_cost": 0,
    "max_token": 128000
  },
  {
    "provider": "Mistral",
    "model": "Mistral Large 2",
    "version": "Large 2 123B",
    "description": "123 B open-weight multilingual model, fluent in dozens of languages and programming languages.",
    "model_size": 123,
    "input_cost": 3.0,
    "output_cost": 9.0,
    "cache_cost": 0,
    "max_token": 128000
  },
  {
    "provider": "Mistral",
    "model": "Mistral Medium",
    "version": "Medium (proprietary)",
    "description": "Cutting-edge multimodal model ideal for professional code, function-calling, and reasoning.",
    "model_size": null,
    "input_cost": 2.75,
    "output_cost": 8.1,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Mistral Small",
    "version": "Small 3.1 24B",
    "description": "Open-weight small model (24 B) with image understanding capabilities (v3.1, released Mar 2025).",
    "model_size": 24,
    "input_cost": 1.0,
    "output_cost": 3.0,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Codestral",
    "version": "Codestral 22B",
    "description": "Code-focused model fluent in 80+ programming languages, open-weight (22 B).",
    "model_size": 22,
    "input_cost": 1.0,
    "output_cost": 3.0,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Mistral Embed",
    "version": "Embed",
    "description": "General-purpose embedding model for text & code.",
    "model_size": null,
    "input_cost": 0.01,
    "output_cost": 0.01,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Mistral 7B",
    "version": "7B v0.1",
    "description": "Apache-2.0 open-source model (7.3 B params), strong on reasoning and code.",
    "model_size": 7.3,
    "input_cost": 0.25,
    "output_cost": 0.25,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Mixtral 8x7B",
    "version": "Mixtral 8×7B",
    "description": "Sparse MoE model with 8 experts, ~46.7 B total (12.9 B per token active).",
    "model_size": 46.7,
    "input_cost": 0.7,
    "output_cost": 0.7,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "Mistral",
    "model": "Mixtral 8x22B",
    "version": "Mixtral 8×22B",
    "description": "Sparse MoE model with 8 experts of 22 B params each (~141 B total).",
    "model_size": 141,
    "input_cost": 2.0,
    "output_cost": 6.0,
    "cache_cost": 0,
    "max_token": null
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑4.1",
    "version": "GPT‑4.1",
    "description": "Flagship GPT‑4.1 model—high capability for complex tasks, coding, long-context.",
    "model_size": null,
    "input_cost": 2.0,
    "output_cost": 8.0,
    "cache_cost": 0.5,
    "max_token": 1000000
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑4.1 mini",
    "version": "GPT‑4.1 mini",
    "description": "Smaller, cost‑efficient variant of GPT‑4.1.",
    "model_size": null,
    "input_cost": 0.4,
    "output_cost": 1.6,
    "cache_cost": 0.1,
    "max_token": 1000000
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑4.1 nano",
    "version": "GPT‑4.1 nano",
    "description": "Fastest, most affordable GPT‑4.1 variant.",
    "model_size": null,
    "input_cost": 0.1,
    "output_cost": 0.4,
    "cache_cost": 0.025,
    "max_token": 1000000
  },
  {
    "provider": "OpenAI",
    "model": "OpenAI o3",
    "version": "o3",
    "description": "Top-tier reasoning model for coding, math, science, vision.",
    "model_size": null,
    "input_cost": 2.0,
    "output_cost": 8.0,
    "cache_cost": 0.5,
    "max_token": null
  },
  {
    "provider": "OpenAI",
    "model": "o4‑mini",
    "version": "o4‑mini",
    "description": "Faster, cost‑efficient reasoning model.",
    "model_size": null,
    "input_cost": 1.1,
    "output_cost": 4.4,
    "cache_cost": 0.275,
    "max_token": null
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑4o",
    "version": "GPT‑4o",
    "description": "Multimodal ‘omni’ model (text, image, audio) with 128k context window.",
    "model_size": null,
    "input_cost": 5.0,
    "output_cost": 20.0,
    "cache_cost": 2.5,
    "max_token": 128000
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑4o mini",
    "version": "GPT‑4o mini",
    "description": "Smaller, cheaper multimodal GPT‑4o variant.",
    "model_size": null,
    "input_cost": 0.6,
    "output_cost": 2.4,
    "cache_cost": 0.3,
    "max_token": 128000
  },
  {
    "provider": "OpenAI",
    "model": "GPT‑image‑1",
    "version": "GPT‑image‑1",
    "description": "Image generation/editing model.",
    "model_size": null,
    "input_cost": 5.0,
    "output_cost": 40.0,
    "cache_cost": 1.25,
    "max_token": null
  },
  {
    "provider": "Claude",
    "model": "Claude Instant",
    "version": "Instant 1.2",
    "description": "Fastest, low‑latency model with modest capabilities.",
    "model_size": null,
    "input_cost": 0.8,
    "output_cost": 2.4,
    "cache_cost": null,
    "max_token": 100000
  },
  {
    "provider": "Claude",
    "model": "Claude 2",
    "version": "Claude 2",
    "description": "Enhanced reasoning model, for more complex tasks than Instant.",
    "model_size": null,
    "input_cost": 8.0,
    "output_cost": 24.0,
    "cache_cost": null,
    "max_token": null
  },
  {
    "provider": "Claude",
    "model": "Claude 3 Haiku",
    "version": "Haiku",
    "description": "Small and fast Claude 3 variant with good baseline performance.",
    "model_size": null,
    "input_cost": 0.25,
    "output_cost": 1.25,
    "cache_cost": null,
    "max_token": null
  },
  {
    "provider": "Claude",
    "model": "Claude 3 Sonnet",
    "version": "Sonnet 3.5 / 3.7",
    "description": "Balanced model for most tasks; hybrid reasoning in 3.7.",
    "model_size": null,
    "input_cost": 3.0,
    "output_cost": 15.0,
    "cache_cost": null,
    "max_token": 200000
  },
  {
    "provider": "Claude",
    "model": "Claude 3 Opus",
    "version": "Opus (Claude 3)",
    "description": "Top-tier Claude 3 model for heavy reasoning and coding.",
    "model_size": null,
    "input_cost": 15.0,
    "output_cost": 75.0,
    "cache_cost": null,
    "max_token": null
  },
  {
    "provider": "Claude",
    "model": "Claude Sonnet 4",
    "version": "Sonnet 4",
    "description": "Latest hybrid‑reasoning Sonnet model; fast plus deep reasoning.",
    "model_size": null,
    "input_cost": 3.0,
    "output_cost": 15.0,
    "cache_cost": null,
    "max_token": null
  },
  {
    "provider": "Claude",
    "model": "Claude Opus 4",
    "version": "Opus 4",
    "description": "Most powerful Claude model, optimized for coding, reasoning, long contexts.",
    "model_size": null,
    "input_cost": 15.0,
    "output_cost": 75.0,
    "cache_cost": null,
    "max_token": null
  }
]
